{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle and Analyze Data\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Gathering Data](#gather)\n",
    "- [Assessing Data](#assess)\n",
    " - [Quality](#quality)\n",
    " - [Tidiness](#tidy)\n",
    "- [Cleaning Data](#clean)\n",
    "- [Analyzing Data](#analyze)\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "This project will illustrate the data wrangling process.  The dataset that is being wrangled comes from the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as WeRateDogs.  WeRateDogs is a Twitter account that rate's people's dogs with a humorous comment about the dog.  These ratings almost always have a denominator of 10 and numerators almost always greater than 10.  With this data, I will try to create interesting and trustworthy analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'gather'></a>\n",
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 pieces of data required for this project.  The first one is the WeRateDogs Twitter archive.  This file is given to us and will be treated like an internal file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second piece of data is tweet image predictions.  The file is hosted on Udacity's servers and will be downloaded programmatically using the Requests library and the following URL: [https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third piece of data will be queried from Twitter's API.  Using the tweet ID's in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt.  Each tweet's JSON data should be written to its own line.  Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'assess'></a>\n",
    "### Assessing Data\n",
    "Detect and document at least eight quality issues and two tidiness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'quality'></a>\n",
    "**Quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'tidy'></a>\n",
    "**Tidiness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'clean'></a>\n",
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'analyze'></a>\n",
    "### Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'conclusion'></a>\n",
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
